<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Creative Coding Audio Visualizer</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <style>
        body { margin: 0; overflow: hidden; }
        #controls { position: absolute; bottom: 20px; left: 50%; transform: translateX(-50%); }
        #fileInput { display: none; }
        #debug { position: absolute; top: 10px; left: 10px; color: white; }
    </style>
</head>
<body>
    <div id="controls">
        <button id="playPause">Play/Pause</button>
        <input type="range" id="volumeSlider" min="0" max="1" step="0.1" value="1">
        <label for="fileInput" class="button">Choose File</label>
        <input type="file" id="fileInput" accept="audio/*">
    </div>
    <div id="debug"></div>

    <script id="fragmentShader" type="x-shader/x-fragment">
       uniform float iTime;
uniform vec2 iResolution;
uniform sampler2D iChannel0;

vec3 palette(float t) {
    vec3 a = vec3(0.5, 0.5, 0.5);
    vec3 b = vec3(0.5, 0.5, 0.5);
    vec3 c = vec3(1.0, 1.0, 1.0);
    vec3 d = vec3(0.263, 0.416, 0.557);

    return a + b * cos(6.28318 * (c * t + d));
}

void mainImage(out vec4 fragColor, in vec2 fragCoord) {
    vec2 uv = (fragCoord * 2.0 - iResolution.xy) / iResolution.y;
    vec2 uv0 = uv;
    vec3 finalColor = vec3(0.0);
    
    // Sample audio at different frequencies
    float audioLow = texture(iChannel0, vec2(0.1, 0.0)).r;
    float audioMid = texture(iChannel0, vec2(0.5, 0.0)).r;
    float audioHigh = texture(iChannel0, vec2(0.9, 0.0)).r;
    
    // Use audio to affect various parameters
    float iterations = 4.0 + audioHigh * 2.0; // Vary complexity based on high frequencies
    float zoomFactor = 1.5 + audioMid * 0.5; // Zoom based on mid frequencies
    float timeScale = 0.4 + audioLow * 0.3; // Time progression affected by low frequencies
    
    for (float i = 0.0; i < iterations; i++) {
        uv = fract(uv * zoomFactor) - 0.5;

        float d = length(uv) * exp(-length(uv0));

        vec3 col = palette(length(uv0) + i * 0.4 + iTime * timeScale);

        d = sin(d * 8.0+ 0.1*timeScale + iTime * (1.0 + audioMid)) / 8.0;
        d = abs(d);

        d = pow(0.01 / d, 1.2 + audioHigh * 0.3);

        finalColor += col * d;
    }
    
    // Apply overall audio reactivity
    float overallAudio = (audioLow + audioMid + audioHigh) / 3.0;
    finalColor *= (1.0 + overallAudio * 2.0);
    
    // Add a subtle pulse effect
    finalColor += vec3(sin(iTime * 2.0 + audioLow * 10.0) * 0.1 * overallAudio);
    
    fragColor = vec4(finalColor, 1.0);
}

void main() {
    mainImage(gl_FragColor, gl_FragCoord.xy);
}
    </script>


    <script>
        let scene, camera, renderer, material, audioContext, analyser, dataArray, audioTexture;
        let isPlaying = false;
        let audio = new Audio();

        function init() {
            scene = new THREE.Scene();
            camera = new THREE.OrthographicCamera(-1, 1, 1, -1, 0, 1);
            renderer = new THREE.WebGLRenderer();
            renderer.setSize(window.innerWidth, window.innerHeight);
            document.body.appendChild(renderer.domElement);

            const geometry = new THREE.PlaneGeometry(2, 2);
            material = new THREE.ShaderMaterial({
                fragmentShader: document.getElementById('fragmentShader').textContent,
                uniforms: {
                    iTime: { value: 0 },
                    iResolution: { value: new THREE.Vector2() },
                    iChannel0: { value: null }
                }
            });
            const mesh = new THREE.Mesh(geometry, material);
            scene.add(mesh);

            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            analyser.fftSize = 256;
            dataArray = new Uint8Array(analyser.frequencyBinCount);

            audioTexture = new THREE.DataTexture(dataArray, 128, 1, THREE.RedFormat);
            material.uniforms.iChannel0.value = audioTexture;

            window.addEventListener('resize', onWindowResize, false);
            document.getElementById('playPause').addEventListener('click', togglePlayPause);
            document.getElementById('volumeSlider').addEventListener('input', changeVolume);
            document.getElementById('fileInput').addEventListener('change', loadAudioFile);

            animate();
        }

        function animate() {
            requestAnimationFrame(animate);
            material.uniforms.iTime.value += 0.01;
            material.uniforms.iResolution.value.set(window.innerWidth, window.innerHeight);

            if (isPlaying) {
                analyser.getByteFrequencyData(dataArray);
                audioTexture.needsUpdate = true;
            }

            renderer.render(scene, camera);

            // Debug info
            const debugElement = document.getElementById('debug');
            debugElement.textContent = `Time: ${material.uniforms.iTime.value.toFixed(2)}, 
                                        Resolution: ${material.uniforms.iResolution.value.x} x ${material.uniforms.iResolution.value.y}, 
                                        Audio: ${isPlaying ? 'Playing' : 'Paused'}`;
        }

        function onWindowResize() {
            renderer.setSize(window.innerWidth, window.innerHeight);
        }

        function togglePlayPause() {
            if (audioContext.state === 'suspended') {
                audioContext.resume();
            }

            if (isPlaying) {
                audio.pause();
                isPlaying = false;
            } else {
                audio.play();
                isPlaying = true;
            }
        }

        function changeVolume() {
            audio.volume = document.getElementById('volumeSlider').value;
        }

        function loadAudioFile(event) {
            const file = event.target.files[0];
            const reader = new FileReader();

            reader.onload = function(e) {
                audio.src = e.target.result;
                const source = audioContext.createMediaElementSource(audio);
                source.connect(analyser);
                analyser.connect(audioContext.destination);
            };

            reader.readAsDataURL(file);
        }

        init();
    </script>
</body>
</html>
