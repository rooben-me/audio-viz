<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>3D Audio Visualizer</title>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/three.js/r128/three.min.js"></script>
    <style>
        body { margin: 0; overflow: hidden; background-color: #000; }
        #controls { position: absolute; bottom: 20px; left: 50%; transform: translateX(-50%); }
        #fileInput { display: none; }
        #debug { position: absolute; top: 10px; left: 10px; color: white; font-family: Arial, sans-serif; }
        button, label { background-color: #4CAF50; border: none; color: white; padding: 10px 20px; text-align: center; text-decoration: none; display: inline-block; font-size: 16px; margin: 4px 2px; cursor: pointer; border-radius: 5px; }
        input[type="range"] { width: 200px; }
    </style>
</head>
<body>
    <div id="controls">
        <button id="playPause">Play/Pause</button>
        <input type="range" id="volumeSlider" min="0" max="1" step="0.1" value="1">
        <label for="fileInput" class="button">Choose File</label>
        <input type="file" id="fileInput" accept="audio/*">
    </div>
    <div id="debug"></div>

    <script id="fragmentShader" type="x-shader/x-fragment">
        uniform float iTime;
        uniform vec2 iResolution;
        uniform sampler2D iChannel0;
        uniform float iChannelTime[4];

        #define st(t1, t2, v1, v2) mix(v1, v2, smoothstep(t1, t2, iTime))
        #define light(d, att) 1. / (1.+pow(abs(d*att), 1.3))

        #define getLevel(x) (texture(iChannel0, vec2(x, 0.0)).r)
        #define logX(x,a,c) (1./(exp(-a*(x-c))+1.))

        float logisticAmp(float amp){
           float c = st(0., 10., .8, 1.), a = 20.;  
           return (logX(amp, a, c) - logX(0.0, a, c)) / (logX(1.0, a, c) - logX(0.0, a, c));
        }
        float getPitch(float freq, float octave){
           freq = pow(2., freq)   * 261.;
           freq = pow(2., octave) * freq / 12000.;
           return logisticAmp(getLevel(freq));
        }
        float getVol(float samples) {
            float avg = 0.;
            for (float i = 0.; i < samples; i++) avg += getLevel(i/samples);
            return avg / samples;
        }

        float sdBox( vec3 p, vec3 b ) {
          vec3 q = abs(p) - b;
          return length(max(q,0.0)) + min(max(q.x,max(q.y,q.z)),0.0);
        }
        float hash13(vec3 p3) {
            p3  = fract(p3 * .1031);
            p3 += dot(p3, p3.zyx + 31.32);
            return fract((p3.x + p3.y) * p3.z);
        }

        void mainImage( out vec4 fragColor, in vec2 fragCoord ) {
            vec2 uv   = (2.*fragCoord-iResolution.xy)/iResolution.y;
            vec3 col  = vec3(0.);
            float vol = getVol(8.);
            
            float hasSound = 1.; // Used only to avoid a black preview image
            if (iChannelTime[0] <= 0.) hasSound = .0;
         
            for (float i = 0., t = 0.; i < 30.; i++) {
                vec3 p  = t*normalize(vec3(uv, 1.));        
                
                vec3 id = floor(abs(p));
                vec3 q  = fract(p)-.5;
                
                float boxRep = sdBox(q, vec3(.3));
                float boxCtn = sdBox(p, vec3(7.5, 6.5, 16.5));

                float dst = max(boxRep, abs(boxCtn) - vol*.2);     
                float freq = smoothstep(16., 0., id.z)*3.*hasSound + hash13(id)*1.5;
               
                col += vec3(.8,.6,1) * (cos(id*.4 + vec3(0,1,2) + iTime) + 2.) 
                     * light(dst, 10. - vol) 
                     * getPitch(freq, 1.);
                
                t += dst;
            }
            
            fragColor = vec4(col,1.0);   
        }

        void main() {
            mainImage(gl_FragColor, gl_FragCoord.xy);
        }
    </script>

    <script>
        let scene, camera, renderer, material, audioContext, analyser, dataArray, audioTexture;
        let isPlaying = false;
        let audio = new Audio();

        function init() {
            scene = new THREE.Scene();
            camera = new THREE.OrthographicCamera(-1, 1, 1, -1, 0, 1);
            renderer = new THREE.WebGLRenderer();
            renderer.setSize(window.innerWidth, window.innerHeight);
            document.body.appendChild(renderer.domElement);

            const geometry = new THREE.PlaneGeometry(2, 2);
            material = new THREE.ShaderMaterial({
                fragmentShader: document.getElementById('fragmentShader').textContent,
                uniforms: {
                    iTime: { value: 0 },
                    iResolution: { value: new THREE.Vector2() },
                    iChannel0: { value: null },
                    iChannelTime: { value: [0, 0, 0, 0] }
                }
            });
            const mesh = new THREE.Mesh(geometry, material);
            scene.add(mesh);

            audioContext = new (window.AudioContext || window.webkitAudioContext)();
            analyser = audioContext.createAnalyser();
            analyser.fftSize = 1024;
            dataArray = new Uint8Array(analyser.frequencyBinCount);

            audioTexture = new THREE.DataTexture(dataArray, 512, 1, THREE.RedFormat);
            material.uniforms.iChannel0.value = audioTexture;

            window.addEventListener('resize', onWindowResize, false);
            document.getElementById('playPause').addEventListener('click', togglePlayPause);
            document.getElementById('volumeSlider').addEventListener('input', changeVolume);
            document.getElementById('fileInput').addEventListener('change', loadAudioFile);

            animate();
        }

        function animate(time) {
            requestAnimationFrame(animate);
            material.uniforms.iTime.value = time * 0.001;
            material.uniforms.iResolution.value.set(window.innerWidth, window.innerHeight);

            if (isPlaying) {
                analyser.getByteFrequencyData(dataArray);
                audioTexture.needsUpdate = true;
                material.uniforms.iChannelTime.value[0] = audio.currentTime;
            }

            renderer.render(scene, camera);

             const debugElement = document.getElementById('debug');
            debugElement.textContent = `Audio: ${isPlaying ? 'Playing' : 'Paused'}, Time: ${audio.currentTime.toFixed(2)}s`;
        }

        function onWindowResize() {
            renderer.setSize(window.innerWidth, window.innerHeight);
        }

        function togglePlayPause() {
            if (audioContext.state === 'suspended') {
                audioContext.resume();
            }

            if (isPlaying) {
                audio.pause();
                isPlaying = false;
            } else {
                audio.play();
                isPlaying = true;
            }
        }

        function changeVolume() {
            audio.volume = document.getElementById('volumeSlider').value;
        }

        function loadAudioFile(event) {
            const file = event.target.files[0];
            const reader = new FileReader();

            reader.onload = function(e) {
                audio.src = e.target.result;
                const source = audioContext.createMediaElementSource(audio);
                source.connect(analyser);
                analyser.connect(audioContext.destination);
                
                // Reset the audio time
                material.uniforms.iChannelTime.value[0] = 0;
            };

            reader.readAsDataURL(file);
        }

        init();
    </script>
</body>
</html>
